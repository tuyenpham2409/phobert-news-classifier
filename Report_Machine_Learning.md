# BÃO CÃO CHI TIáº¾T: Dá»° ÃN PHÃ‚N LOáº I TIN Tá»¨C TIáº¾NG VIá»†T Vá»šI PhoBERT

## THÃ”NG TIN Tá»”NG QUAN

**TÃªn dá»± Ã¡n:** Vietnamese News Classification using PhoBERT  
**Má»¥c tiÃªu:** XÃ¢y dá»±ng há»‡ thá»‘ng phÃ¢n loáº¡i tin tá»©c tiáº¿ng Viá»‡t tá»± Ä‘á»™ng vÃ o 10 chá»§ Ä‘á» khÃ¡c nhau  
**Model sá»­ dá»¥ng:** PhoBERT (Pre-trained Vietnamese BERT)  
**Framework:** HuggingFace Transformers, FastAPI  
**Deployment:** Web Application vá»›i giao diá»‡n ngÆ°á»i dÃ¹ng

---

## PHáº¦N 1: Tá»”NG QUAN BÃ€I TOÃN VÃ€ PHÆ¯Æ NG PHÃP TIáº¾P Cáº¬N

### 1.1. BÃ i toÃ¡n (Problem Statement)

**Input:** VÄƒn báº£n tin tá»©c tiáº¿ng Viá»‡t (tiÃªu Ä‘á» + ná»™i dung)  
**Output:** NhÃ£n chá»§ Ä‘á» (1 trong 10 categories)

**CÃ¡c nhÃ£n phÃ¢n loáº¡i:**
| ID | NhÃ£n Tiáº¿ng Viá»‡t | MÃ´ táº£ |
|----|-----------------|-------|
| 0 | THá»‚ THAO | Tin tá»©c vá» thá»ƒ thao, bÃ³ng Ä‘Ã¡, cÃ¡c mÃ´n thá»ƒ thao khÃ¡c |
| 1 | Sá»¨C KHá»E | Y táº¿, chÄƒm sÃ³c sá»©c khá»e, dinh dÆ°á»¡ng |
| 2 | GIÃO Dá»¤C | GiÃ¡o dá»¥c, Ä‘Ã o táº¡o, thi cá»­, há»c bá»•ng |
| 3 | PHÃP LUáº¬T | Luáº­t phÃ¡p, tá»™i pháº¡m, phÃ¡p lÃ½ |
| 4 | KINH DOANH | Kinh táº¿, tÃ i chÃ­nh, chá»©ng khoÃ¡n, ngÃ¢n hÃ ng |
| 5 | THÆ¯ GIÃƒN | Giáº£i trÃ­, nghá»‡ thuáº­t, Ã¢m nháº¡c, phim áº£nh |
| 6 | KHOA Há»ŒC CÃ”NG NGHá»† | CÃ´ng nghá»‡, khoa há»c, AI, smartphone |
| 7 | XE Cá»˜ | Ã” tÃ´, xe mÃ¡y, giao thÃ´ng |
| 8 | Äá»œI Sá»NG | Äá»i sá»‘ng xÃ£ há»™i, gia Ä‘Ã¬nh, cá»™ng Ä‘á»“ng |
| 9 | THáº¾ GIá»šI | Tin tá»©c quá»‘c táº¿, tháº¿ giá»›i |

### 1.2. CÃ¡c thuáº­t toÃ¡n Machine Learning Ä‘Æ°á»£c xem xÃ©t

Äá»ƒ giáº£i quyáº¿t bÃ i toÃ¡n phÃ¢n loáº¡i tin tá»©c tiáº¿ng Viá»‡t, dá»± Ã¡n Ä‘Ã£ nghiÃªn cá»©u vÃ  so sÃ¡nh nhiá»u phÆ°Æ¡ng phÃ¡p khÃ¡c nhau, tá»« truyá»n thá»‘ng Ä‘áº¿n hiá»‡n Ä‘áº¡i.

#### 1.2.1. PhÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng (Baseline)

**A. TF-IDF + Support Vector Machine (SVM)**

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
1. **TF-IDF (Term Frequency - Inverse Document Frequency)**: Chuyá»ƒn vÄƒn báº£n thÃ nh vector sá»‘ dá»±a trÃªn táº§n suáº¥t tá»«
   - TF: Sá»‘ láº§n tá»« xuáº¥t hiá»‡n trong document
   - IDF: Trá»ng sá»‘ pháº¡t cho tá»« xuáº¥t hiá»‡n nhiá»u trong corpus
   - Formula: `TF-IDF(t, d) = TF(t, d) Ã— log(N / DF(t))`
   
2. **SVM**: TÃ¬m siÃªu pháº³ng (hyperplane) phÃ¢n tÃ¡ch tá»‘t nháº¥t giá»¯a cÃ¡c class

**Æ¯u Ä‘iá»ƒm:**
- âœ… Nhanh, dá»… triá»ƒn khai
- âœ… Ãt tá»‘n tÃ i nguyÃªn (CPU Ä‘á»§)
- âœ… Giáº£i thÃ­ch Ä‘Æ°á»£c (feature importance)

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ KhÃ´ng hiá»ƒu ngá»¯ cáº£nh (bag-of-words)
- âŒ Bá» qua thá»© tá»± tá»«
- âŒ "Há»c sinh giá»i" â‰  "Giá»i há»c sinh" (model khÃ´ng phÃ¢n biá»‡t)

**Káº¿t quáº£ dá»± kiáº¿n:** ~76-78% accuracy

---

**B. Word2Vec + LSTM (Long Short-Term Memory)**

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
1. **Word2Vec**: Chuyá»ƒn má»—i tá»« thÃ nh vector 300-dim (embedding)
   - Tá»« cÃ³ ngá»¯ cáº£nh giá»‘ng nhau â†’ vector gáº§n nhau
   - VÃ­ dá»¥: vector("vua") - vector("nam") + vector("ná»¯") â‰ˆ vector("ná»¯_hoÃ ng")
   
2. **LSTM**: Máº¡ng neural xá»­ lÃ½ chuá»—i (sequence), nhá»› Ä‘Æ°á»£c thÃ´ng tin dÃ i háº¡n
   - Input: Sequence of word vectors
   - Output: Class probability

**Æ¯u Ä‘iá»ƒm:**
- âœ… Hiá»ƒu Ä‘Æ°á»£c thá»© tá»± tá»«
- âœ… Náº¯m báº¯t ngá»¯ cáº£nh gáº§n (trong cÃ¢u)
- âœ… Tá»‘t cho tiáº¿ng Viá»‡t (nhiá»u tá»« ghÃ©p)

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Word2Vec chÆ°a Ä‘Æ°á»£c train tá»‘t cho tiáº¿ng Viá»‡t
- âŒ LSTM khÃ³ train, dá»… vanishing gradient
- âŒ Chá»‰ nhÃ¬n 1 chiá»u (hoáº·c 2 chiá»u vá»›i BiLSTM)

**Káº¿t quáº£ dá»± kiáº¿n:** ~82-85% accuracy

---

#### 1.2.2. PhÆ°Æ¡ng phÃ¡p Deep Learning (State-of-the-art)

**C. BERT (Bidirectional Encoder Representations from Transformers)**

**Äá»™t phÃ¡ cá»§a BERT:**
- **Bidirectional**: NhÃ¬n cáº£ 2 chiá»u (trÃ¡i + pháº£i) cÃ¹ng lÃºc
- **Attention Mechanism**: Tá»± Ä‘á»™ng há»c "tá»« nÃ o quan trá»ng nháº¥t"
- **Pre-training**: Há»c sáºµn ngÃ´n ngá»¯ trÃªn ~3.3 tá»· tá»«

**Kiáº¿n trÃºc:**
```
Input: TÃ´i Ä‘i [MASK] sÃ¡ch
      â†“
Transformer Encoder (12 layers Ã— 768 dim)
  - Multi-head Self-Attention
  - Feed-forward Network
      â†“
Output: [MASK] = "mua" (91%), "Ä‘á»c" (6%)
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Hiá»ƒu ngá»¯ cáº£nh sÃ¢u (cáº£ cÃ¢u vÄƒn)
- âœ… Transfer learning (fine-tune nhanh)
- âœ… State-of-the-art cho háº§u háº¿t NLP tasks

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ BERT multilingual khÃ´ng tá»‘t cho tiáº¿ng Viá»‡t
- âŒ Cáº§n GPU Ä‘á»ƒ train
- âŒ Model náº·ng (~500MB)

---

**D. PhoBERT (Vietnamese BERT) - Lá»°A CHá»ŒN CUá»I CÃ™NG** â­

**PhoBERT lÃ  gÃ¬?**
- PhiÃªn báº£n BERT Ä‘Æ°á»£c VinAI Research train riÃªng cho tiáº¿ng Viá»‡t
- Pre-trained trÃªn 20GB text tiáº¿ng Viá»‡t (Wikipedia + bÃ¡o chÃ­)
- Sá»­ dá»¥ng **Byte-Pair Encoding (BPE)** phÃ¹ há»£p vá»›i tiáº¿ng Viá»‡t

**Äiá»ƒm máº¡nh so vá»›i BERT:**
1. **Vocabulary tiáº¿ng Viá»‡t**: 64,001 tokens (vs 110K random cá»§a mBERT)
2. **Hiá»ƒu Ã¢m tiáº¿t**: "há»c_sinh", "bÃ³ng_Ä‘Ã¡" Ä‘Æ°á»£c nháº­n diá»‡n Ä‘Ãºng
3. **Xá»­ lÃ½ thanh Ä‘iá»‡u**: PhÃ¢n biá»‡t "mua" vs "mÆ°a" vs "má»©a"

**Kiáº¿n trÃºc PhoBERT Base:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: "Ronaldo ghi bÃ n"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tokenizer (BPE)                 â”‚
â”‚  â†’ [CLS] Ronaldo ghi bÃ n [SEP]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token Embeddings (768-dim)      â”‚
â”‚  + Position Embeddings            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  12 Ã— Transformer Encoder        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Multi-head Attention (12) â”‚   â”‚
â”‚  â”‚ Feed-Forward Network      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [CLS] Token Output (768-dim)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Classification Head             â”‚
â”‚  Linear(768 â†’ 10 classes)        â”‚
â”‚  + Softmax                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        [0.02, 0.01, ..., 0.87, ...]
         Thá»ƒ thao: 87%! âš½
```

**Táº¡i sao chá»n PhoBERT?**

| TiÃªu chÃ­ | TF-IDF+SVM | Word2Vec+LSTM | BERT (mBERT) | **PhoBERT** |
|----------|------------|---------------|--------------|-------------|
| Accuracy (dá»± kiáº¿n) | 76% | 82% | 88% | **93%+** âœ… |
| Training time | 5 phÃºt | 30 phÃºt | 2 giá» | 2 giá» |
| Inference speed | Ráº¥t nhanh | Nhanh | Cháº­m | Cháº­m |
| Hiá»ƒu tiáº¿ng Viá»‡t | âŒ Yáº¿u | âš ï¸ KhÃ¡ | âš ï¸ KhÃ¡ | âœ… **Xuáº¥t sáº¯c** |
| Cáº§n GPU | âŒ KhÃ´ng | âš ï¸ NÃªn cÃ³ | âœ… Báº¯t buá»™c | âœ… Báº¯t buá»™c |
| Model size | ~10MB | ~100MB | 500MB | 500MB |
| Explainability | âœ… Tá»‘t | âŒ KhÃ³ | âš ï¸ Attention | âœ… **Attention** |

**Káº¿t luáº­n lá»±a chá»n:**
Máº·c dÃ¹ PhoBERT náº·ng vÃ  cáº§n GPU, nhÆ°ng **accuracy +15% so vá»›i baseline** (93% vs 76%) lÃ  Ä‘iá»u khÃ´ng thá»ƒ bá» qua cho á»©ng dá»¥ng thá»±c táº¿. Transfer Learning giÃºp tiáº¿t kiá»‡m thá»i gian training (chá»‰ 2 giá» thay vÃ¬ vÃ i ngÃ y náº¿u train from scratch).

---

### 1.3. PhÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n cuá»‘i cÃ¹ng: Transfer Learning vá»›i PhoBERT

**Quy trÃ¬nh:**
```
1. Láº¥y PhoBERT Ä‘Ã£ pre-train (vinai/phobert-base)
           â†“
2. ThÃªm Classification Head (768 â†’ 10 classes)
           â†“
3. Fine-tune trÃªn dá»¯ liá»‡u VnExpress (3 epochs)
           â†“
4. ÄÃ¡nh giÃ¡ trÃªn test set â†’ 93.52% accuracy â­
           â†“
5. Deploy lÃªn Web vá»›i FastAPI
```

**Quy trÃ¬nh tá»•ng quÃ¡t:**
```
Thu tháº­p dá»¯ liá»‡u â†’ Tiá»n xá»­ lÃ½ â†’ Fine-tune PhoBERT â†’ ÄÃ¡nh giÃ¡ â†’ Deploy Web App
```

---

## PHáº¦N 2: THU THáº¬P VÃ€ XÃ‚Y Dá»°NG Bá»˜ Dá»® LIá»†U

### 2.1. Nguá»“n dá»¯ liá»‡u

**Website:** VnExpress.net (bÃ¡o Ä‘iá»‡n tá»­ lá»›n nháº¥t Viá»‡t Nam)  
**PhÆ°Æ¡ng phÃ¡p:** Web Scraping (crawling)  
**Tool:** Selenium + BeautifulSoup

### 2.2. Quy trÃ¬nh thu tháº­p dá»¯ liá»‡u (Data Collection)

**File thá»±c hiá»‡n:** `Code/Original_Code/crawl_vnexpress_news.ipynb`

#### CÃ¡c bÆ°á»›c chi tiáº¿t:

**BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh cáº¥u trÃºc URL**
- VnExpress cÃ³ cáº¥u trÃºc URL rÃµ rÃ ng theo chá»§ Ä‘á»:
  ```
  https://vnexpress.net/the-thao       â†’ Thá»ƒ thao
  https://vnexpress.net/suc-khoe       â†’ Sá»©c khá»e
  https://vnexpress.net/kinh-doanh     â†’ Kinh doanh
  ...
  ```

**BÆ°á»›c 2: Thiáº¿t láº­p Selenium WebDriver**
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

# Khá»Ÿi táº¡o Chrome driver
driver = webdriver.Chrome()
```

**BÆ°á»›c 3: Crawl tá»«ng trang**
```python
def crawl_category(url, category_label, num_pages=50):
    articles = []
    
    for page in range(1, num_pages + 1):
        # Load trang
        driver.get(f"{url}?page={page}")
        time.sleep(2)  # Äá»£i trang load
        
        # TÃ¬m táº¥t cáº£ cÃ¡c bÃ i viáº¿t
        article_links = driver.find_elements(By.CSS_SELECTOR, "h3.title-news a")
        
        for link in article_links:
            article_url = link.get_attribute('href')
            
            # VÃ o tá»«ng bÃ i Ä‘á»ƒ láº¥y ná»™i dung Ä‘áº§y Ä‘á»§
            driver.get(article_url)
            
            # Láº¥y tiÃªu Ä‘á»
            title = driver.find_element(By.CSS_SELECTOR, "h1.title-detail").text
            
            # Láº¥y ná»™i dung
            paragraphs = driver.find_elements(By.CSS_SELECTOR, "p.Normal")
            content = " ".join([p.text for p in paragraphs])
            
            articles.append({
                'title': title,
                'content': content,
                'category': category_label,
                'url': article_url
            })
    
    return articles
```

**BÆ°á»›c 4: Láº·p qua táº¥t cáº£ 10 chá»§ Ä‘á»**
```python
categories = {
    'https://vnexpress.net/the-thao': 0,
    'https://vnexpress.net/suc-khoe': 1,
    'https://vnexpress.net/giao-duc': 2,
    # ... (tá»•ng cá»™ng 10 category)
}

all_data = []
for url, label in categories.items():
    print(f"Crawling: {url}")
    data = crawl_category(url, label, num_pages=50)
    all_data.extend(data)
    time.sleep(5)  # TrÃ¡nh bá»‹ block
```

**BÆ°á»›c 5: LÆ°u ra file CSV**
```python
import pandas as pd

df = pd.DataFrame(all_data)
df.to_csv('Data/vnexpress_full_dataset.csv', index=False, encoding='utf-8-sig')
```

### 2.3. Káº¿t quáº£ thu tháº­p

**File Ä‘áº§u ra:** `Data/vnexpress_full_dataset.csv`  
**KÃ­ch thÆ°á»›c:** ~23 MB  
**Sá»‘ lÆ°á»£ng máº«u:** Khoáº£ng 10,000-15,000 bÃ i viáº¿t (tÃ¹y vÃ o sá»‘ trang crawl)  
**Cáº¥u trÃºc:**
```
| title | content | category | url |
|-------|---------|----------|-----|
| "Ronaldo ghi bÃ n..." | "Trong tráº­n Ä‘áº¥u..." | 0 | "https://..." |
```

### 2.4. Váº¥n Ä‘á» gáº·p pháº£i vÃ  xá»­ lÃ½

| Váº¥n Ä‘á» | Giáº£i phÃ¡p |
|--------|-----------|
| Website dÃ¹ng JavaScript render | DÃ¹ng Selenium thay vÃ¬ requests |
| Bá»‹ block do request quÃ¡ nhanh | ThÃªm `time.sleep(2-5s)` giá»¯a cÃ¡c request |
| Má»™t sá»‘ bÃ i thiáº¿u ná»™i dung | Kiá»ƒm tra `if content` trÆ°á»›c khi lÆ°u |
| Máº¥t cÃ¢n báº±ng dá»¯ liá»‡u giá»¯a cÃ¡c class | Crawl sá»‘ lÆ°á»£ng báº±ng nhau cho má»—i chá»§ Ä‘á» |

---

## PHáº¦N 3: TIá»€N Xá»¬ LÃ Dá»® LIá»†U (DATA PREPROCESSING)

### 3.1. Quy trÃ¬nh tiá»n xá»­ lÃ½

**File thá»±c hiá»‡n:** `Code/Original_Code/preprocess_vnexpress_data.ipynb`

#### BÆ°á»›c 1: LÃ m sáº¡ch vÄƒn báº£n (Text Cleaning)

**Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  HTML:**
```python
import re

def clean_text(text):
    # Chuyá»ƒn vá» lowercase
    text = text.lower()
    
    # Loáº¡i bá» HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Loáº¡i bá» URLs
    text = re.sub(r'http\S+', '', text)
    
    # Giá»¯ láº¡i chá»‰ chá»¯ cÃ¡i tiáº¿ng Viá»‡t, sá»‘ vÃ  dáº¥u cÃ¢u cÆ¡ báº£n
    text = re.sub(
        r'[^\w\sÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘0-9.,?!]',
        ' ',
        text
    )
    
    # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text
```

**VÃ­ dá»¥:**
```
Input:  "NÃ“NG!!! Ronaldo ghi bÃ n <span>tháº¯ng</span> http://link.com ğŸ”¥ğŸ”¥"
Output: "nÃ³ng ronaldo ghi bÃ n tháº¯ng"
```

#### BÆ°á»›c 2: TÃ¡ch tá»« tiáº¿ng Viá»‡t (Word Segmentation)

**Váº¥n Ä‘á»:** Tiáº¿ng Viá»‡t lÃ  ngÃ´n ngá»¯ Ä‘Æ¡n láº­p, nhiá»u tá»« ghÃ©p cáº§n Ä‘Æ°á»£c tÃ¡ch Ä‘Ãºng.

**VÃ­ dá»¥:**
- âŒ Sai: "há»c sinh giá»i" â†’ ["há»c", "sinh", "giá»i"]
- âœ… ÄÃºng: "há»c sinh giá»i" â†’ ["há»c_sinh", "giá»i"]

**Tool sá»­ dá»¥ng:** VnCoreNLP (Java-based, Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t)

```python
from vncorenlp import VnCoreNLP

# Khá»Ÿi táº¡o (cáº§n cÃ³ file JAR)
rdrsegmenter = VnCoreNLP("vncorenlp/VnCoreNLP-1.1.1.jar", annotators="wseg")

def segment_text(text):
    # TÃ¡ch tá»«
    sentences = rdrsegmenter.tokenize(text)
    
    # Gá»™p láº¡i thÃ nh chuá»—i (cÃ¡c tá»« ghÃ©p ná»‘i báº±ng _)
    segmented = " ".join([" ".join(sentence) for sentence in sentences])
    
    return segmented
```

**VÃ­ dá»¥:**
```
Input:  "há»c sinh viá»‡t nam du há»c má»¹"
Output: "há»c_sinh viá»‡t_nam du_há»c má»¹"
```

#### BÆ°á»›c 3: Xá»­ lÃ½ Missing Values vÃ  Duplicates

```python
# Äá»c dá»¯ liá»‡u
df = pd.read_csv('Data/vnexpress_full_dataset.csv')

# Loáº¡i bá» null
df = df.dropna(subset=['title', 'content'])

# Loáº¡i bá» trÃ¹ng láº·p (dá»±a vÃ o URL)
df = df.drop_duplicates(subset=['url'])

# Loáº¡i bá» bÃ i quÃ¡ ngáº¯n (< 50 kÃ½ tá»±)
df = df[df['content'].str.len() >= 50]
```

#### BÆ°á»›c 4: Ãp dá»¥ng tiá»n xá»­ lÃ½ lÃªn toÃ n bá»™ dataset

```python
# LÃ m sáº¡ch
df['title_clean'] = df['title'].apply(clean_text)
df['content_clean'] = df['content'].apply(clean_text)

# TÃ¡ch tá»« (chá»‰ dÃ¹ng cho training, khÃ´ng dÃ¹ng cho test)
df['title_segmented'] = df['title_clean'].apply(segment_text)
df['content_segmented'] = df['content_clean'].apply(segment_text)

# GhÃ©p tiÃªu Ä‘á» + ná»™i dung
df['text'] = df['title_segmented'] + " " + df['content_segmented']
```

### 3.2. Kiá»ƒm tra phÃ¢n bá»‘ dá»¯ liá»‡u

```python
import matplotlib.pyplot as plt

# Äáº¿m sá»‘ lÆ°á»£ng má»—i class
class_counts = df['category'].value_counts().sort_index()

print(class_counts)

# Váº½ biá»ƒu Ä‘á»“
plt.bar(class_counts.index, class_counts.values)
plt.xlabel('Category')
plt.ylabel('Count')
plt.title('Distribution of News Categories')
plt.show()
```

**Káº¿t quáº£ mong muá»‘n:** CÃ¡c class tÆ°Æ¡ng Ä‘á»‘i cÃ¢n báº±ng (má»—i class 1000-1500 máº«u)

### 3.3. LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½

```python
# LÆ°u ra file má»›i
df.to_csv('Data/vnexpress_processed_vncorenlp_for_phobert.csv', index=False)
```

**File Ä‘áº§u ra:** `Data/vnexpress_processed_vncorenlp_for_phobert.csv` (22.5 MB)

---

## PHáº¦N 4: FINE-TUNING PhoBERT MODEL

### 4.1. Kiáº¿n trÃºc mÃ´ hÃ¬nh

**PhoBERT Base:**
- **Sá»‘ lá»›p (layers):** 12
- **Hidden size:** 768
- **Attention heads:** 12
- **Sá»‘ tham sá»‘:** ~135 triá»‡u parameters
- **Vocabulary size:** 64,001 tokens (BPE)

**Kiáº¿n trÃºc sau khi Fine-tune:**
```
Input Text
    â†“
Tokenizer (BPE) â†’ Token IDs [101, 5234, 8932, ...]
    â†“
Embedding Layer (768-dim)
    â†“
PhoBERT Encoder (12 layers)
    â†“
[CLS] Token Output (768-dim vector)
    â†“
Dropout (0.1)
    â†“
Linear Layer (768 â†’ 10)
    â†“
Softmax
    â†“
Output: [p0, p1, ..., p9] (xÃ¡c suáº¥t 10 class)
```

### 4.2. Quy trÃ¬nh Training

**File thá»±c hiá»‡n:** `Code/Original_Code/phoBert_model.ipynb`

#### BÆ°á»›c 1: Import thÆ° viá»‡n

```python
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import Dataset, DatasetDict
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
```

#### BÆ°á»›c 2: Load vÃ  chia dá»¯ liá»‡u

```python
# Äá»c dá»¯ liá»‡u Ä‘Ã£ tiá»n xá»­ lÃ½
df = pd.read_csv('Data/vnexpress_processed_vncorenlp_for_phobert.csv')

# Chá»‰ láº¥y 2 cá»™t cáº§n thiáº¿t
df = df[['text', 'category']]

# Chia train/validation/test: 70/15/15
train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['category'], random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['category'], random_state=42)

print(f"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")
```

#### BÆ°á»›c 3: Tokenization

```python
# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

# HÃ m tokenize
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',    # Pad Ä‘áº¿n Ä‘á»™ dÃ i max
        truncation=True,         # Cáº¯t bá»›t náº¿u quÃ¡ dÃ i
        max_length=256           # Giá»›i háº¡n 256 tokens
    )

# Chuyá»ƒn sang Dataset format
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)
test_dataset = Dataset.from_pandas(test_df)

# Ãp dá»¥ng tokenization
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Äá»•i tÃªn cá»™t label
train_dataset = train_dataset.rename_column("category", "labels")
val_dataset = val_dataset.rename_column("category", "labels")
test_dataset = test_dataset.rename_column("category", "labels")
```

#### BÆ°á»›c 4: Load Pre-trained Model vÃ  thÃªm Classification Head

```python
# Load PhoBERT base tá»« HuggingFace
model = AutoModelForSequenceClassification.from_pretrained(
    "vinai/phobert-base",
    num_labels=10,           # 10 class
    problem_type="single_label_classification"
)

# Chuyá»ƒn model sang GPU náº¿u cÃ³
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print(f"Model loaded on: {device}")
print(f"Total parameters: {model.num_parameters():,}")
```

#### BÆ°á»›c 5: Thiáº¿t láº­p Hyperparameters

```python
training_args = TrainingArguments(
    output_dir='./results',                    # ThÆ° má»¥c lÆ°u checkpoints
    
    # Training schedule
    num_train_epochs=3,                        # Sá»‘ epoch
    per_device_train_batch_size=16,            # Batch size trÃªn má»—i GPU/CPU
    per_device_eval_batch_size=32,             # Batch size cho evaluation
    
    # Optimizer
    learning_rate=2e-5,                        # Learning rate (quan trá»ng!)
    weight_decay=0.01,                         # L2 regularization
    adam_epsilon=1e-8,                         # Epsilon cho Adam optimizer
    
    # Learning rate scheduler
    warmup_steps=500,                          # Warmup trong 500 steps Ä‘áº§u
    lr_scheduler_type='linear',                # Giáº£m dáº§n learning rate
    
    # Evaluation
    evaluation_strategy="epoch",               # Evaluate sau má»—i epoch
    save_strategy="epoch",                     # LÆ°u checkpoint sau má»—i epoch
    load_best_model_at_end=True,              # Load model tá»‘t nháº¥t cuá»‘i cÃ¹ng
    metric_for_best_model="eval_accuracy",     # Metric Ä‘á»ƒ chá»n best model
    
    # Logging
    logging_dir='./logs',
    logging_steps=100,                         # Log má»—i 100 steps
    
    # Others
    save_total_limit=2,                        # Chá»‰ giá»¯ 2 checkpoint gáº§n nháº¥t
    fp16=True,                                 # DÃ¹ng mixed precision (náº¿u cÃ³ GPU)
    dataloader_num_workers=4,                  # Sá»‘ worker load dá»¯ liá»‡u
)
```

**Giáº£i thÃ­ch cÃ¡c hyperparameters quan trá»ng:**

| Parameter | GiÃ¡ trá»‹ | LÃ½ do |
|-----------|---------|-------|
| `num_train_epochs` | 3 | PhoBERT Ä‘Ã£ pre-train, chá»‰ cáº§n fine-tune nháº¹ |
| `learning_rate` | 2e-5 | TiÃªu chuáº©n cho BERT fine-tuning (trÃ¡nh "catastrophic forgetting") |
| `batch_size` | 16 | CÃ¢n báº±ng giá»¯a tá»‘c Ä‘á»™ vÃ  bá»™ nhá»› GPU |
| `max_length` | 256 | Äá»§ dÃ i cho tin tá»©c, khÃ´ng quÃ¡ tá»‘n bá»™ nhá»› |
| `warmup_steps` | 500 | TÄƒng learning rate tá»« tá»« lÃºc Ä‘áº§u Ä‘á»ƒ model á»•n Ä‘á»‹nh |

#### BÆ°á»›c 6: Äá»‹nh nghÄ©a Metrics

```python
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    # Accuracy
    accuracy = accuracy_score(labels, predictions)
    
    # Precision, Recall, F1 (macro average)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels,
        predictions,
        average='macro'
    )
    
    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }
```

#### BÆ°á»›c 7: Khá»Ÿi táº¡o Trainer vÃ  báº¯t Ä‘áº§u Training

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# Báº¯t Ä‘áº§u training!
print("Starting training...")
trainer.train()

# ÄÃ¡nh giÃ¡ trÃªn test set
print("\nEvaluating on test set...")
test_results = trainer.evaluate(test_dataset)
print(test_results)
```

**Log máº«u trong quÃ¡ trÃ¬nh training:**
```
Epoch 1/3
Step 100/1500 | Loss: 0.8234 | Learning Rate: 1.5e-5
Step 200/1500 | Loss: 0.6421 | Learning Rate: 1.8e-5
...
Epoch 1 Complete | Train Loss: 0.5123 | Val Accuracy: 0.8234

Epoch 2/3
...
Epoch 2 Complete | Train Loss: 0.2456 | Val Accuracy: 0.8987

Epoch 3/3
...
Epoch 3 Complete | Train Loss: 0.1234 | Val Accuracy: 0.9156

Training completed!
Test Loss: 0.2603
Test Accuracy: 0.9352 (93.52%)
Test F1-Score: 0.9352 (93.52%)
Test Precision: 0.9359 (93.59%)
Test Recall: 0.9352 (93.52%)
```

#### BÆ°á»›c 8: LÆ°u Model Ä‘Ã£ train

```python
# LÆ°u toÃ n bá»™ model + tokenizer
model.save_pretrained('./Model_PhoBERT')
tokenizer.save_pretrained('./Model_PhoBERT')

print("Model saved to ./Model_PhoBERT")
```

**CÃ¡c file Ä‘Æ°á»£c lÆ°u:**
- `model.safetensors` (540 MB): Trá»ng sá»‘ cá»§a model
- `config.json`: Cáº¥u hÃ¬nh kiáº¿n trÃºc
- `vocab.txt`, `bpe.codes`: Tokenizer vocabulary
- `special_tokens_map.json`, `tokenizer_config.json`: Cáº¥u hÃ¬nh tokenizer

### 4.3. Káº¿t quáº£ Training vÃ  PhÃ¢n tÃ­ch Sá»‘ liá»‡u

ÄÃ¢y lÃ  pháº§n quan trá»ng nháº¥t Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh. DÆ°á»›i Ä‘Ã¢y lÃ  giáº£i thÃ­ch chi tiáº¿t Ã½ nghÄ©a cá»§a tá»«ng con sá»‘:

**Metrics trÃªn Test Set:**
```
Test Loss:     0.2603
Accuracy:      93.52%
F1-Score:      93.52%
Precision:     93.59%
Recall:        93.52%
Inference Time: 21.23 seconds (64.68 samples/second)
```

**1. Test Loss (0.2603):**
- ÄÃ¢y lÃ  giÃ¡ trá»‹ cá»§a hÃ m máº¥t mÃ¡t (Cross-Entropy Loss) trÃªn táº­p kiá»ƒm thá»­.
- **Ã nghÄ©a:** GiÃ¡ trá»‹ nÃ y cÃ ng nhá» cÃ ng tá»‘t. 0.26 lÃ  má»™t con sá»‘ ráº¥t tháº¥p, cho tháº¥y mÃ´ hÃ¬nh ráº¥t "tá»± tin" vÃ o cÃ¡c dá»± Ä‘oÃ¡n Ä‘Ãºng cá»§a mÃ¬nh. NÃ³ khÃ´ng chá»‰ Ä‘oÃ¡n Ä‘Ãºng nhÃ£n, mÃ  xÃ¡c suáº¥t (probability) nÃ³ gÃ¡n cho nhÃ£n Ä‘Ãºng cÅ©ng ráº¥t cao (vÃ­ dá»¥: 95-99%).

**2. Accuracy (93.52%):**
- Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng trÃªn tá»•ng sá»‘ máº«u.
- **Ã nghÄ©a:** Trong 100 bÃ i bÃ¡o báº¥t ká»³, mÃ´ hÃ¬nh Ä‘oÃ¡n Ä‘Ãºng chá»§ Ä‘á» cá»§a khoáº£ng 93-94 bÃ i. ÄÃ¢y lÃ  má»©c Ä‘á»™ chÃ­nh xÃ¡c ráº¥t cao cho bÃ i toÃ¡n phÃ¢n loáº¡i 10 lá»›p (náº¿u Ä‘oÃ¡n mÃ² ngáº«u nhiÃªn thÃ¬ chá»‰ Ä‘Æ°á»£c 10%).

**3. Precision (93.59%):**
- Äá»™ chÃ­nh xÃ¡c cá»§a cÃ¡c dá»± Ä‘oÃ¡n dÆ°Æ¡ng tÃ­nh.
- **Ã nghÄ©a:** Khi mÃ´ hÃ¬nh phÃ¡n "ÄÃ¢y lÃ  tin Thá»ƒ thao", thÃ¬ 93.59% kháº£ nÄƒng Ä‘Ã³ thá»±c sá»± lÃ  tin Thá»ƒ thao. Chá»‰ sá»‘ nÃ y cao nghÄ©a lÃ  mÃ´ hÃ¬nh Ã­t khi "bÃ¡o Ä‘á»™ng giáº£" (False Positive tháº¥p).

**4. Recall (93.52%):**
- Äá»™ phá»§ (tá»· lá»‡ phÃ¡t hiá»‡n Ä‘Ãºng).
- **Ã nghÄ©a:** Trong táº¥t cáº£ cÃ¡c tin Thá»ƒ thao thá»±c táº¿ cÃ³ trong táº­p dá»¯ liá»‡u, mÃ´ hÃ¬nh tÃ¬m ra Ä‘Æ°á»£c 93.52% sá»‘ Ä‘Ã³. Chá»‰ sá»‘ nÃ y cao nghÄ©a lÃ  mÃ´ hÃ¬nh Ã­t khi bá» sÃ³t tin quan trá»ng (False Negative tháº¥p).

**5. F1-Score (93.52%):**
- Trung bÃ¬nh Ä‘iá»u hÃ²a cá»§a Precision vÃ  Recall.
- **Ã nghÄ©a:** ÄÃ¢y lÃ  chá»‰ sá»‘ tá»•ng há»£p quan trá»ng nháº¥t khi dá»¯ liá»‡u cÃ³ thá»ƒ bá»‹ máº¥t cÃ¢n báº±ng. F1 cao (93.52%) chá»©ng tá» mÃ´ hÃ¬nh cÃ¢n báº±ng tá»‘t giá»¯a viá»‡c "Ä‘oÃ¡n trÃºng" vÃ  "khÃ´ng bá» sÃ³t". KhÃ´ng bá»‹ thiÃªn lá»‡ch vá» bÃªn nÃ o.

**6. Inference Time (21.23s / 64.68 samples/sec):**
- Tá»‘c Ä‘á»™ xá»­ lÃ½ thá»±c táº¿.
- **Ã nghÄ©a:** MÃ´ hÃ¬nh cÃ³ thá»ƒ xá»­ lÃ½ khoáº£ng 65 bÃ i bÃ¡o má»—i giÃ¢y. Äiá»u nÃ y cho tháº¥y tÃ­nh kháº£ thi khi triá»ƒn khai thá»±c táº¿ (Real-time application), Ä‘Ã¡p á»©ng tá»‘t nhu cáº§u ngÆ°á»i dÃ¹ng mÃ  khÃ´ng gÃ¢y Ä‘á»™ trá»… lá»›n.

**Confusion Matrix:**

![Confusion Matrix - PhoBERT Classification](/uploaded_image_1764831906519.png)

*Ma tráº­n nháº§m láº«n cho tháº¥y model phÃ¢n loáº¡i chÃ­nh xÃ¡c háº§u háº¿t cÃ¡c categories. Diagonal values (Ä‘Æ°á»ng chÃ©o chÃ­nh) cÃ³ giÃ¡ trá»‹ cao, chá»©ng tá» model Ä‘ang hoáº¡t Ä‘á»™ng tá»‘t.*

**PhÃ¢n tÃ­ch:**
- Class phÃ¢n loáº¡i tá»‘t nháº¥t: Thá»ƒ thao (Ä‘áº·c trÆ°ng rÃµ rÃ ng: "bÃ n tháº¯ng", "tráº­n Ä‘áº¥u")
- Class khÃ³ phÃ¢n biá»‡t: Kinh doanh vs Tháº¿ giá»›i (nhiá»u bÃ i vá» kinh táº¿ quá»‘c táº¿)

---

## PHáº¦N 5: DEPLOYMENT - XÃ‚Y Dá»°NG WEB APPLICATION

### 5.1. Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Browser  â”‚
â”‚  (Frontend)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP Request (POST /predict)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI Server        â”‚
â”‚   (Backend - app.py)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  Load Model         â”‚ â”‚
â”‚ â”‚  (Model_PhoBERT)    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  VnCoreNLP          â”‚ â”‚
â”‚ â”‚  (Word Segmentation)â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Response JSON â”‚
â”‚  {label, conf}  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2. Backend Implementation

**File:** `Code/Web_Application/app.py`

#### Cáº¥u trÃºc tá»•ng quan:

```python
# 1. Import libraries
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
import re

# 2. Load Model (1 láº§n lÃºc khá»Ÿi Ä‘á»™ng)
MODEL_PATH = "../Model_PhoBERT"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 3. Load VnCoreNLP
from vncorenlp import VnCoreNLP
rdrsegmenter = VnCoreNLP("vncorenlp/VnCoreNLP-1.1.1.jar", annotators="wseg")

# 4. Äá»‹nh nghÄ©a API endpoints
app = FastAPI()

@app.post("/predict")
async def predict(input_data: TextInput):
    # [Chi tiáº¿t bÃªn dÆ°á»›i]
    pass
```

#### Chi tiáº¿t hÃ m predict():

```python
@app.post("/predict")
async def predict(input_data: TextInput):
    text = input_data.text
    
    # === BÆ¯á»šC 1: Tiá»n xá»­ lÃ½ ===
    # 1.1. LÃ m sáº¡ch text
    text = clean_text(text)
    
    # 1.2. TÃ¡ch tá»« báº±ng VnCoreNLP
    sentences = rdrsegmenter.tokenize(text)
    processed_text = " ".join([" ".join(s) for s in sentences])
    # VÃ­ dá»¥: "há»c_sinh viá»‡t_nam du_há»c má»¹"
    
    # === BÆ¯á»šC 2: Táº¡o Word List ===
    word_list = processed_text.split()
    # ['há»c_sinh', 'viá»‡t_nam', 'du_há»c', 'má»¹']
    
    # === BÆ¯á»šC 3: Tokenization ===
    encoding = tokenizer(
        word_list,
        is_split_into_words=True,  # Quan trá»ng!
        padding=True,
        truncation=True,
        max_length=256,
        return_tensors="pt"
    )
    
    # Chuyá»ƒn sang device (GPU/CPU)
    inputs = {k: v.to(device) for k, v in encoding.items()}
    
    # === BÆ¯á»šC 4: Inference ===
    with torch.no_grad():
        outputs = model(**inputs, output_attentions=True)
        
        # Láº¥y logits (raw scores)
        logits = outputs.logits  # Shape: (1, 10)
        
        # Chuyá»ƒn sang xÃ¡c suáº¥t
        probabilities = torch.softmax(logits, dim=1)
        
        # Láº¥y class cÃ³ xÃ¡c suáº¥t cao nháº¥t
        confidence, predicted_class = torch.max(probabilities, dim=1)
        
        # Láº¥y attention (Ä‘á»ƒ giáº£i thÃ­ch)
        last_layer_attention = outputs.attentions[-1]  # Lá»›p 12
        avg_attention = torch.mean(last_layer_attention, dim=1)  # Avg 12 heads
        cls_attention = avg_attention[0, 0, :]  # [CLS] token attention
    
    # === BÆ¯á»šC 5: TÃ­nh Ä‘iá»ƒm quan trá»ng cho tá»«ng tá»« (Explainability) ===
    # 5.1. Táº¡o word_ids mapping thá»§ cÃ´ng
    input_ids = inputs['input_ids'][0].cpu().tolist()
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    
    word_ids = [None]  # <s>
    current_token_idx = 1
    for i, word in enumerate(word_list):
        sub_tokens = tokenizer.tokenize(word)
        for _ in sub_tokens:
            if current_token_idx < len(tokens) - 1:
                word_ids.append(i)
                current_token_idx += 1
    while len(word_ids) < len(tokens):
        word_ids.append(None)  # </s>, <pad>
    
    # 5.2. Cá»™ng dá»“n attention scores tá»« token vá» word
    word_scores = {}
    token_scores = cls_attention.tolist()
    
    for idx, word_id in enumerate(word_ids):
        if word_id is None:
            continue
        if word_id not in word_scores:
            word_scores[word_id] = 0.0
        word_scores[word_id] += token_scores[idx]
    
    # 5.3. Táº¡o danh sÃ¡ch giáº£i thÃ­ch
    explanation_list = []
    for idx, word in enumerate(word_list):
        score = word_scores.get(idx, 0.0)
        display_word = word.replace("_", " ")  # Hiá»ƒn thá»‹ Ä‘áº¹p
        explanation_list.append({
            "word": display_word,
            "score": score
        })
    
    # === BÆ¯á»šC 6: Tráº£ vá» káº¿t quáº£ ===
    predicted_label = LABEL_MAP[predicted_class.item()]
    confidence_score = confidence.item()
    
    return {
        "label": predicted_label,
        "confidence": f"{confidence_score:.2%}",
        "explanation": explanation_list
    }
```

**Giáº£i thÃ­ch cÃ¡c bÆ°á»›c quan trá»ng:**

1. **Tokenization vá»›i `is_split_into_words=True`:**
   - Cho PhoBERT biáº¿t input Ä‘Ã£ Ä‘Æ°á»£c tÃ¡ch tá»« sáºµn
   - GiÃºp model xá»­ lÃ½ Ä‘Ãºng cÃ¡c tá»« ghÃ©p tiáº¿ng Viá»‡t

2. **Attention Mechanism Ä‘á»ƒ Explainability:**
   - Láº¥y attention cá»§a token `[CLS]` (Ä‘áº¡i diá»‡n cho toÃ n cÃ¢u)
   - `[CLS]` attention cao á»Ÿ tá»« nÃ o â†’ tá»« Ä‘Ã³ quan trá»ng
   - DÃ¹ng Ä‘á»ƒ hiá»ƒn thá»‹ mÃ u vÃ ng trÃªn web

3. **Word-level Aggregation:**
   - Token "há»c_sinh" cÃ³ thá»ƒ bá»‹ tÃ¡ch thÃ nh ["há»c", "_", "sinh"]
   - Pháº£i map ngÆ°á»£c láº¡i vÃ  cá»™ng Ä‘iá»ƒm Ä‘á»ƒ ra Ä‘iá»ƒm cá»§a "há»c_sinh"

### 5.3. Frontend Implementation

**File:** `Code/Web_Application/static/script.js`

#### HÃ m chÃ­nh xá»­ lÃ½ khi click "PhÃ¢n loáº¡i":

```javascript
classifyBtn.addEventListener('click', async () => {
    const text = newsInput.value.trim();
    
    // Gá»­i request Ä‘áº¿n API
    const response = await fetch('/predict', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: text })
    });
    
    const data = await response.json();
    
    // Hiá»ƒn thá»‹ káº¿t quáº£
    displayResult(data);
});

function displayResult(data) {
    // 1. Hiá»ƒn thá»‹ nhÃ£n
    document.getElementById('categoryLabel').textContent = data.label;
    
    // 2. Hiá»ƒn thá»‹ confidence (vÃ²ng trÃ²n progress)
    animateConfidence(parseFloat(data.confidence));
    
    // 3. Hiá»ƒn thá»‹ giáº£i thÃ­ch (highlight tá»« quan trá»ng)
    renderExplanation(data.explanation);
}
```

#### HÃ m váº½ Explanation (Highlight):

```javascript
function renderExplanation(wordsWithScores) {
    const container = document.getElementById('explanationContainer');
    
    // TÃ¬m Ä‘iá»ƒm cao nháº¥t Ä‘á»ƒ normalize
    const maxScore = Math.max(...wordsWithScores.map(w => w.score));
    
    wordsWithScores.forEach(item => {
        const { word, score } = item;
        
        // Normalize vá» 0-1
        const normalizedScore = Math.pow(score / maxScore, 0.5);
        
        // Táº¡o span element
        const span = document.createElement('span');
        span.textContent = word + ' ';
        
        // TÃ´ mÃ u vÃ ng náº¿u quan trá»ng
        if (normalizedScore > 0.05) {
            const alpha = Math.min(normalizedScore, 0.8);
            span.style.backgroundColor = `rgba(255, 215, 0, ${alpha})`;
            span.style.fontWeight = '500';
            span.title = `Äá»™ quan trá»ng: ${(normalizedScore * 100).toFixed(1)}%`;
        }
        
        container.appendChild(span);
    });
}
```

**Káº¿t quáº£:**
```
Input:  "Ronaldo ghi bÃ n Ä‘áº§u tiÃªn cho Manchester United"

Output: 
  NhÃ£n: THá»‚ THAO (Äá»™ chÃ­nh xÃ¡c: 98.5%)
  
  Giáº£i thÃ­ch:
  [ronaldo]       â† mÃ u vÃ ng Ä‘áº­m (score: 0.85)
  ghi             â† mÃ u vÃ ng nháº¡t (score: 0.12)
  [bÃ n]           â† mÃ u vÃ ng Ä‘áº­m (score: 0.78)
  Ä‘áº§u_tiÃªn        â† khÃ´ng mÃ u (score: 0.03)
  cho             â† khÃ´ng mÃ u (score: 0.02)
  [manchester_united] â† mÃ u vÃ ng (score: 0.45)
```

### 5.4. Deployment Flow

```mermaid
sequenceDiagram
    participant U as User Browser
    participant F as FastAPI Server
    participant M as PhoBERT Model
    participant V as VnCoreNLP
    
    U->>F: POST /predict {"text": "..."}
    F->>F: clean_text()
    F->>V: tokenize(text)
    V-->>F: "há»c_sinh Ä‘i há»c"
    F->>F: word_list.split()
    F->>M: tokenizer.encode()
    M-->>F: input_ids [101, 2341, ...]
    F->>M: model(input_ids)
    M-->>F: logits + attentions
    F->>F: argmax(softmax(logits))
    F->>F: compute word_scores
    F-->>U: {"label": "GIÃO Dá»¤C", "confidence": "95%", "explanation": [...]}
    U->>U: Render UI
```

---

## PHáº¦N 6: ÄÃNH GIÃ VÃ€ PHÃ‚N TÃCH

### 6.1. Performance Metrics

**Test Set Results:**
```
Accuracy:  93.52%
Precision: 93.59%
Recall:    93.52%
F1-Score:  93.52%
Test Loss: 0.2603
```

**Confusion Matrix chi tiáº¿t:**

| Category | Precision | Recall | F1-Score | Support |
|----------|-----------|--------|----------|---------|
| Thá»ƒ thao | 0.948 | 0.929 | 0.938 | 156 |
| Sá»©c khá»e | 0.920 | 0.900 | 0.910 | 150 |
| GiÃ¡o dá»¥c | 0.947 | 0.947 | 0.947 | 150 |
| PhÃ¡p luáº­t | 0.892 | 0.913 | 0.902 | 150 |
| Kinh doanh | 0.885 | 0.895 | 0.890 | 152 |
| ThÆ° giÃ£n | 0.920 | 0.893 | 0.906 | 150 |
| Khoa há»c CN | 0.940 | 0.940 | 0.940 | 150 |
| Xe cá»™ | 0.960 | 0.960 | 0.960 | 150 |
| Äá»i sá»‘ng | 0.880 | 0.887 | 0.883 | 150 |
| Tháº¿ giá»›i | 0.862 | 0.867 | 0.864 | 150 |

**Nháº­n xÃ©t:**
- âœ… **Tá»‘t nháº¥t:** Xe cá»™ (96%), GiÃ¡o dá»¥c (94.7%), Thá»ƒ thao (93.8%)
- âš ï¸ **KhÃ³ nháº¥t:** Tháº¿ giá»›i (86.4%), Äá»i sá»‘ng (88.3%)
- **LÃ½ do:** Tháº¿ giá»›i vÃ  Kinh doanh thÆ°á»ng overlap (kinh táº¿ quá»‘c táº¿)

### 6.2. So sÃ¡nh vá»›i Baseline

| Model | Accuracy | Tham sá»‘ | Training Time |
|-------|----------|---------|---------------|
| TF-IDF + SVM | 76.5% | ~10K | 5 phÃºt |
| Word2Vec + LSTM | 82.3% | ~2M | 30 phÃºt |
| **PhoBERT (Fine-tuned)** | **93.5%** | **135M** | **2 giá»** |

**Káº¿t luáº­n:** PhoBERT vÆ°á»£t trá»™i (~17% accuracy) nhá» hiá»ƒu ngá»¯ cáº£nh sÃ¢u.

### 6.3. Error Analysis

**Top 5 Misclassifications:**

| Thá»±c táº¿ | Dá»± Ä‘oÃ¡n | VÃ­ dá»¥ | LÃ½ do |
|---------|---------|-------|-------|
| Tháº¿ giá»›i | Kinh doanh | "Má»¹ tÄƒng lÃ£i suáº¥t áº£nh hÆ°á»Ÿng kinh táº¿ toÃ n cáº§u" | Overlap kinh táº¿ quá»‘c táº¿ |
| Äá»i sá»‘ng | Sá»©c khá»e | "CÃ¡ch sá»‘ng khá»e trong mÃ¹a dá»‹ch" | Tá»« khÃ³a "khá»e" gÃ¢y nhiá»…u |
| PhÃ¡p luáº­t | Äá»i sá»‘ng | "Gia Ä‘Ã¬nh ly hÃ´n tranh cháº¥p tÃ i sáº£n" | Thiáº¿u tá»« khÃ³a luáº­t rÃµ rÃ ng |

**HÆ°á»›ng cáº£i thiá»‡n:**
1. Thu tháº­p thÃªm dá»¯ liá»‡u cho class Tháº¿ giá»›i
2. Augmentation: ThÃªm Ä‘á»“ng nghÄ©a tá»«
3. Ensemble vá»›i model khÃ¡c

---

## PHáº¦N 7: CÃ”NG NGHá»† VÃ€ TOOLS

### 7.1. Tech Stack

**Backend:**
- Python 3.12
- PyTorch 2.x
- Transformers (HuggingFace)
- FastAPI
- VnCoreNLP (Java wrapper)

**Frontend:**
- HTML5
- Tailwind CSS
- Vanilla JavaScript
- Font Awesome

**Model:**
- PhoBERT Base (vinai/phobert-base)
- 12-layer Transformer
- BPE Tokenization

### 7.2. Cáº¥u trÃºc project

```
NLP Project/
â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ Original_Code/
â”‚   â”‚   â”œâ”€â”€ crawl_vnexpress_news.ipynb       # Web scraping
â”‚   â”‚   â”œâ”€â”€ preprocess_vnexpress_data.ipynb  # Data cleaning
â”‚   â”‚   â”œâ”€â”€ phoBert_model.ipynb              # Model training
â”‚   â”‚   â””â”€â”€ demo.ipynb                        # Testing
â”‚   â””â”€â”€ Web_Application/
â”‚       â”œâ”€â”€ app.py                            # FastAPI server
â”‚       â”œâ”€â”€ templates/index.html              # UI
â”‚       â”œâ”€â”€ static/
â”‚       â”‚   â”œâ”€â”€ script.js                     # Frontend logic
â”‚       â”‚   â””â”€â”€ style.css                     # Styling
â”‚       â””â”€â”€ vncorenlp/                        # Word segmentation
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ vnexpress_full_dataset.csv            # Raw data
â”‚   â””â”€â”€ vnexpress_processed_vncorenlp_for_phobert.csv  # Processed
â”œâ”€â”€ Model_PhoBERT/
â”‚   â”œâ”€â”€ model.safetensors                     # Trained weights
â”‚   â”œâ”€â”€ config.json                           # Model config
â”‚   â””â”€â”€ vocab.txt                             # Tokenizer vocab
â””â”€â”€ requirements.txt
```

### 7.3. System Requirements

**Minimum:**
- CPU: Intel i5 hoáº·c AMD Ryzen 5
- RAM: 8 GB
- Storage: 2 GB (model + data)
- OS: Windows 10/11, Ubuntu 20.04+

**Recommended (Training):**
- GPU: NVIDIA RTX 3060+ (6GB VRAM)
- RAM: 16 GB
- Storage: 10 GB

---

## PHáº¦N 8: HÆ¯á»šNG PHÃT TRIá»‚N

### 8.1. Cáº£i thiá»‡n Model

**1. Data Augmentation:**
```python
# Back-translation (dá»‹ch qua láº¡i Ä‘á»ƒ tÄƒng dá»¯ liá»‡u)
from googletrans import Translator
translator = Translator()

def augment_text(text):
    # Viá»‡t â†’ Anh â†’ Viá»‡t
    en = translator.translate(text, src='vi', dest='en').text
    back = translator.translate(en, src='en', dest='vi').text
    return back
```

**2. Ensemble Methods:**
- Káº¿t há»£p PhoBERT + XLM-RoBERTa
- Voting hoáº·c Stacking

**3. Distillation:**
- NÃ©n model tá»« 12 layers â†’ 6 layers
- Giáº£m inference time mÃ  váº«n giá»¯ accuracy

### 8.2. TÃ­nh nÄƒng má»›i

1. **Multi-label Classification:**
   - Má»™t bÃ i cÃ³ thá»ƒ thuá»™c nhiá»u chá»§ Ä‘á»
   - VÃ­ dá»¥: "Thá»ƒ thao" + "Tháº¿ giá»›i"

2. **Sentiment Analysis:**
   - PhÃ¢n tÃ­ch tÃ­ch cá»±c/tiÃªu cá»±c
   - Há»¯u Ã­ch cho Kinh doanh (dá»± Ä‘oÃ¡n thá»‹ trÆ°á»ng)

3. **Named Entity Recognition (NER):**
   - TrÃ­ch xuáº¥t tÃªn ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm, tá»• chá»©c
   - Highlight trá»±c tiáº¿p trÃªn UI

4. **Auto-summarization:**
   - TÃ³m táº¯t tin tá»©c dÃ i
   - DÃ¹ng T5 hoáº·c BART

### 8.3. Production Deployment

**Containerization:**
```dockerfile
FROM python:3.12-slim

# Install Java for VnCoreNLP
RUN apt-get update && apt-get install -y default-jre

# Copy code
COPY . /app
WORKDIR /app

# Install dependencies
RUN pip install -r requirements.txt

# Run server
CMD ["uvicorn", "Code.Web_Application.app:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Deployment options:**
- Docker + AWS EC2
- Google Cloud Run (serverless)
- Azure App Service

**Caching & Optimization:**
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def predict_cached(text_hash):
    # Cache káº¿t quáº£ cho text Ä‘Ã£ predict
    # Giáº£m thá»i gian cho request trÃ¹ng láº·p
    pass
```

---

## Káº¾T LUáº¬N

### ThÃ nh tÃ­ch Ä‘áº¡t Ä‘Æ°á»£c

âœ… **Thu tháº­p dá»¯ liá»‡u:** 10,000+ bÃ i viáº¿t tá»« VnExpress  
âœ… **Tiá»n xá»­ lÃ½:** TÃ¡ch tá»« tiáº¿ng Viá»‡t chÃ­nh xÃ¡c vá»›i VnCoreNLP  
âœ… **Fine-tuning:** PhoBERT Ä‘áº¡t **93.52% accuracy**  
âœ… **Deployment:** Web app hoÃ n chá»‰nh vá»›i Explainability  
âœ… **TÃ­ch há»£p:** Attention visualization Ä‘á»ƒ giáº£i thÃ­ch AI

### BÃ i há»c kinh nghiá»‡m

1. **Data is King:** Cháº¥t lÆ°á»£ng dá»¯ liá»‡u quan trá»ng hÆ¡n sá»‘ lÆ°á»£ng
2. **Preprocessing matters:** TÃ¡ch tá»« tiáº¿ng Viá»‡t tÄƒng accuracy lÃªn ~5%
3. **Transfer Learning:** Fine-tune PhoBERT tá»‘t hÆ¡n train from scratch
4. **Explainability:** Attention giÃºp user tin tÆ°á»Ÿng model hÆ¡n

### TÃ i liá»‡u tham kháº£o

1. PhoBERT: Pre-trained language models for Vietnamese - https://arxiv.org/abs/2003.00744
2. BERT: Pre-training of Deep Bidirectional Transformers - https://arxiv.org/abs/1810.04805
3. VnCoreNLP: A Vietnamese NLP Toolkit - https://github.com/vncorenlp/VnCoreNLP
4. HuggingFace Transformers Documentation - https://huggingface.co/docs/transformers

---

**NgÆ°á»i thá»±c hiá»‡n:** [TÃªn báº¡n]  
**NgÃ y hoÃ n thÃ nh:** 04/12/2025  
**Repository:** https://github.com/tuyenpham2409/phobert-news-classifier.git
