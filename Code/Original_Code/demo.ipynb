{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\n","from google.colab import drive\n","import os\n","\n","# Mount Drive (Ä‘á»ƒ láº¥y Model PhoBERT xá»‹n)\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","!pip install streamlit transformers torch vncorenlp -q\n","\n","# Táº£i cÃ´ng cá»¥ Cloudflare (Thay tháº¿ Ngrok)\n","!wget -q -O cloudflared-linux-amd64 https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n","!chmod +x cloudflared-linux-amd64\n","\n","# Táº£i VnCoreNLP (Báº¯t buá»™c Ä‘á»ƒ cháº¡y app)\n","!mkdir -p vncorenlp/models/wordsegmenter\n","!wget -q -P vncorenlp/ https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","!wget -q -P vncorenlp/models/wordsegmenter/ https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","!wget -q -P vncorenlp/models/wordsegmenter/ https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4SlXNvXKiEV6","executionInfo":{"status":"ok","timestamp":1764207033891,"user_tz":-420,"elapsed":36134,"user":{"displayName":"TuyÃªn Pháº¡m","userId":"00807895422532063883"}},"outputId":"01180f12-4037-4a47-d662-b1ac3c936db9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["app_code = \"\"\"\n","import streamlit as st\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from vncorenlp import VnCoreNLP\n","import re\n","\n","# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN\n","# Báº¡n kiá»ƒm tra ká»¹ Ä‘Æ°á»ng dáº«n nÃ y trong Drive cá»§a báº¡n nhÃ©\n","MODEL_PATH = \"/content/drive/My Drive/NLP Project/Model_PhoBERT\"\n","VNCORENLP_PATH = \"vncorenlp/VnCoreNLP-1.1.1.jar\"\n","\n","id2label = {\n","    0: \"THá»‚ THAO âš½\", 1: \"Sá»¨C KHá»E ğŸ¥\", 2: \"GIÃO Dá»¤C ğŸ“\",\n","    3: \"PHÃP LUáº¬T âš–ï¸\", 4: \"KINH DOANH ğŸ’°\", 5: \"THÆ¯ GIÃƒN ğŸ˜‚\", 6: \"KHOA Há»ŒC CÃ”NG NGHá»† ğŸ–¥ï¸\", 7: \"XE Cá»˜ ğŸï¸\",\n","    8: \"Äá»œI Sá»NG ğŸ’â€â™‚ï¸\", 9: \"THáº¾ GIá»šI ğŸŒ\"\n","}\n","\n","@st.cache_resource\n","def load_resources():\n","    try:\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n","        rdrsegmenter = VnCoreNLP(VNCORENLP_PATH, annotators=\"wseg\", max_heap_size='-Xmx500m')\n","        return tokenizer, model, rdrsegmenter\n","    except Exception as e:\n","        return None, None, None\n","\n","tokenizer, model, rdrsegmenter = load_resources()\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'<.*?>', '', text)\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub(r'[^\\w\\sÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘0-9.,?!]', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","def preprocess(text):\n","    text = clean_text(text)\n","    try:\n","        sentences = rdrsegmenter.tokenize(text)\n","        return \" \".join([\" \".join(sentence) for sentence in sentences])\n","    except:\n","        return text\n","\n","st.title(\"ğŸ“° PHÃ‚N LOáº I TIN Tá»¨C (PhoBERT)\")\n","st.write(\"DÃ¡n ná»™i dung bÃ i bÃ¡o vÃ o Ä‘Ã¢y:\")\n","input_text = st.text_area(\"\", height=150)\n","\n","if st.button(\"PhÃ¢n loáº¡i\"):\n","    if not tokenizer:\n","        st.error(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y Model táº¡i: {MODEL_PATH}. HÃ£y kiá»ƒm tra láº¡i Drive!\")\n","    elif not input_text.strip():\n","        st.warning(\"ChÆ°a nháº­p ná»™i dung!\")\n","    else:\n","        with st.spinner(\"Äang phÃ¢n tÃ­ch...\"):\n","            processed_text = preprocess(input_text)\n","            inputs = tokenizer(processed_text, return_tensors=\"pt\", truncation=True, max_length=256)\n","            with torch.no_grad():\n","                outputs = model(**inputs)\n","            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","            pred_idx = torch.argmax(probs).item()\n","            confidence = probs[0][pred_idx].item()\n","\n","            st.success(f\"Káº¿t quáº£: **{id2label[pred_idx]}**\")\n","            st.info(f\"Äá»™ tin cáº­y: {confidence*100:.2f}%\")\n","            with st.expander(\"Chi tiáº¿t xá»­ lÃ½\"):\n","                st.code(processed_text)\n","\"\"\"\n","\n","# Ghi ná»™i dung vÃ o file app.py\n","with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(app_code)\n","\n","print(\" ÄÃ£ táº¡o file app.py thÃ nh cÃ´ng!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BF9_Navi2Wc","executionInfo":{"status":"ok","timestamp":1764207033917,"user_tz":-420,"elapsed":9,"user":{"displayName":"TuyÃªn Pháº¡m","userId":"00807895422532063883"}},"outputId":"438d0d5a-310e-4f3b-ef8f-620ff586b2eb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":[" ÄÃ£ táº¡o file app.py thÃ nh cÃ´ng!\n"]},{"output_type":"stream","name":"stderr","text":["<>:34: SyntaxWarning: invalid escape sequence '\\S'\n","<>:34: SyntaxWarning: invalid escape sequence '\\S'\n","/tmp/ipython-input-67298072.py:34: SyntaxWarning: invalid escape sequence '\\S'\n","  text = re.sub(r'http\\S+', '', text)\n"]}]},{"cell_type":"code","source":["import subprocess\n","import time\n","\n","!streamlit run app.py &>/content/logs.txt &\n","time.sleep(5)\n","\n","print(\" Äang táº¡o Ä‘Æ°á»ng háº§m Cloudflare...\")\n","!nohup ./cloudflared-linux-amd64 tunnel --url http://localhost:8501 > tunnel.log 2>&1 &\n","time.sleep(8)\n","\n","\n","!grep -o 'https://.*\\.trycloudflare.com' tunnel.log | head -n 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkq9E72Ti7OO","executionInfo":{"status":"ok","timestamp":1764207085886,"user_tz":-420,"elapsed":13336,"user":{"displayName":"TuyÃªn Pháº¡m","userId":"00807895422532063883"}},"outputId":"8fb76faf-e18e-49bf-d541-d9973eb6b244"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":[" Äang táº¡o Ä‘Æ°á»ng háº§m Cloudflare...\n","https://thick-verbal-interests-advocacy.trycloudflare.com\n"]}]}]}